import json
import os

def process_chunks_json(input_file="chunks_json/datasets-chunks-2.json", output_dir="processed_dataset"):
    """
    å¤„ç†text-chunks.jsonæ–‡ä»¶ï¼Œå°†summaryä¸contentç»„åˆç”Ÿæˆé¢„è®­ç»ƒæ•°æ®é›†
    
    Args:
        input_file: è¾“å…¥çš„JSONæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
    """
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    # è¯»å–JSONæ–‡ä»¶
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        print(f"æˆåŠŸè¯»å– {input_file}ï¼ŒåŒ…å« {len(chunks_data)} ä¸ªchunks")
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {input_file}")
        return
    except json.JSONDecodeError as e:
        print(f"é”™è¯¯ï¼šJSONè§£æå¤±è´¥ - {e}")
        return
    
    # å¤„ç†æ•°æ®é›†çš„ä¸åŒæ ¼å¼
    processed_data = []
    
    # 1. ç”Ÿæˆç»„åˆæ ¼å¼ï¼ˆsummary + contentï¼‰
    combined_dataset = []
    for idx, chunk in enumerate(chunks_data):
        combined_text = f"{chunk.get('summary', '')}\n\n{chunk.get('content', '')}"
        
        combined_item = {
            "text": combined_text,
        }
        combined_dataset.append(combined_item)
    
    # 2. ç”Ÿæˆé—®ç­”æ ¼å¼
    qa_dataset = []
    for idx, chunk in enumerate(chunks_data):
        qa_item = {
            "id": idx + 1,
            "question": f"è¯·æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼š\n{chunk.get('content', '')}",
            "answer": chunk.get('summary', ''),
            "source": chunk.get('fileName', ''),
            "name": chunk.get('name', '')
        }
        qa_dataset.append(qa_item)
    
    # 3. ç”Ÿæˆçº¯æ–‡æœ¬æ ¼å¼ï¼ˆç”¨äºè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒï¼‰
    text_dataset = []
    for idx, chunk in enumerate(chunks_data):
        # æ ¼å¼1ï¼šç›´æ¥ç»„åˆ
        text_item1 = {
            "id": f"{idx + 1}_combined",
            "text": f"{chunk.get('summary', '')}\n\n{chunk.get('content', '')}"
        }
        
        # æ ¼å¼2ï¼šç»“æ„åŒ–ç»„åˆ
        text_item2 = {
            "id": f"{idx + 1}_structured",
            "text": f"æ–‡æ¡£: {chunk.get('fileName', '')}\næ‘˜è¦: {chunk.get('summary', '')}\n\nè¯¦ç»†å†…å®¹:\n{chunk.get('content', '')}"
        }
        
        text_dataset.extend([text_item1, text_item2])
    
    # ä¿å­˜ä¸åŒæ ¼å¼çš„æ•°æ®é›†
    datasets = {
        "combined_dataset.json": combined_dataset,
        "qa_dataset.json": qa_dataset,
        "text_dataset.json": text_dataset
    }
    
    for filename, dataset in datasets.items():
        output_path = os.path.join(output_dir, filename)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        print(f"å·²ä¿å­˜ {filename}: {len(dataset)} æ¡è®°å½•")
    
    # ç”Ÿæˆçº¯æ–‡æœ¬æ–‡ä»¶ï¼ˆç”¨äºæŸäº›é¢„è®­ç»ƒæ¡†æ¶ï¼‰
    text_output_path = os.path.join(output_dir, "training_text.txt")
    with open(text_output_path, 'w', encoding='utf-8') as f:
        for chunk in chunks_data:
            # å†™å…¥ç»“æ„åŒ–æ–‡æœ¬
            f.write(f"=== {chunk.get('fileName', 'Unknown')} ===\n")
            f.write(f"æ‘˜è¦: {chunk.get('summary', '')}\n\n")
            f.write(f"{chunk.get('content', '')}\n")
            f.write("\n" + "="*80 + "\n\n")
    
    print(f"å·²ä¿å­˜çº¯æ–‡æœ¬æ–‡ä»¶: training_text.txt")
    
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    stats = {
        "total_chunks": len(chunks_data),
        "total_characters": sum(len(chunk.get('content', '') + chunk.get('summary', '')) for chunk in chunks_data),
        "average_size": sum(chunk.get('size', 0) for chunk in chunks_data) / len(chunks_data) if chunks_data else 0,
        "unique_files": len(set(chunk.get('fileName', '') for chunk in chunks_data)),
        "projects": list(set(chunk.get('projectId', '') for chunk in chunks_data))
    }
    
    stats_path = os.path.join(output_dir, "dataset_stats.json")
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print(f"\n=== æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ ===")
    print(f"æ€»chunksæ•°é‡: {stats['total_chunks']}")
    print(f"æ€»å­—ç¬¦æ•°: {stats['total_characters']:,}")
    print(f"å¹³å‡å¤§å°: {stats['average_size']:.1f}")
    print(f"å”¯ä¸€æ–‡ä»¶æ•°: {stats['unique_files']}")
    print(f"é¡¹ç›®æ•°: {len(stats['projects'])}")
    print(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_path}")

def generate_training_samples(input_file="chunks_json/text-chunks.json", output_file="processed_dataset/training_samples.jsonl"):
    """
    ç”Ÿæˆé€‚åˆæœºå™¨å­¦ä¹ è®­ç»ƒçš„æ ·æœ¬æ ¼å¼ï¼ˆJSONLæ ¼å¼ï¼‰
    """
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {input_file}")
        return
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for idx, chunk in enumerate(chunks_data):
            # åˆ›å»ºå¤šç§è®­ç»ƒæ ·æœ¬æ ¼å¼
            
            # æ ·æœ¬1ï¼šæ‘˜è¦ç”Ÿæˆä»»åŠ¡
            sample1 = {
                "instruction": "è¯·ä¸ºä»¥ä¸‹å†…å®¹ç”Ÿæˆæ‘˜è¦ï¼š",
                "input": chunk.get('content', ''),
                "output": chunk.get('summary', ''),
                "task_type": "summarization"
            }
            f.write(json.dumps(sample1, ensure_ascii=False) + '\n')
            
            # æ ·æœ¬2ï¼šå†…å®¹ç†è§£ä»»åŠ¡
            sample2 = {
                "instruction": "æ ¹æ®æ‘˜è¦ï¼Œè¿™ä¸ªæ–‡æ¡£å¯èƒ½åŒ…å«ä»€ä¹ˆè¯¦ç»†å†…å®¹ï¼Ÿ",
                "input": chunk.get('summary', ''),
                "output": chunk.get('content', ''),
                "task_type": "content_expansion"
            }
            f.write(json.dumps(sample2, ensure_ascii=False) + '\n')
            
            # æ ·æœ¬3ï¼šæ–‡æ¡£åˆ†ç±»ä»»åŠ¡ï¼ˆåŸºäºæ–‡ä»¶åï¼‰
            file_type = chunk.get('fileName', '').split('_')[0] if '_' in chunk.get('fileName', '') else 'general'
            sample3 = {
                "instruction": "è¿™ä¸ªæ–‡æ¡£å±äºä»€ä¹ˆç±»å‹ï¼Ÿ",
                "input": f"{chunk.get('summary', '')}\n\n{chunk.get('content', '')}",
                "output": file_type,
                "task_type": "classification"
            }
            f.write(json.dumps(sample3, ensure_ascii=False) + '\n')
    
    print(f"å·²ç”Ÿæˆè®­ç»ƒæ ·æœ¬æ–‡ä»¶: {output_file}")

def load_and_validate_dataset(dataset_path="processed_dataset/combined_dataset.json"):
    """
    åŠ è½½å¹¶éªŒè¯å¤„ç†åçš„æ•°æ®é›†ï¼Œä¸ºåç»­è®­ç»ƒåšå‡†å¤‡
    """
    print(f"\n=== åŠ è½½è®­ç»ƒæ•°æ®é›† ===")
    
    try:
        with open(dataset_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
        
        print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®é›†: {dataset_path}")
        print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(dataset)} æ¡è®°å½•")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_chars = sum(len(item['text']) for item in dataset)
        avg_length = total_chars / len(dataset) if dataset else 0
        
        print(f"ğŸ“ æ€»å­—ç¬¦æ•°: {total_chars:,}")
        print(f"ğŸ“ å¹³å‡é•¿åº¦: {avg_length:.1f} å­—ç¬¦/æ¡")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬
        print(f"\nğŸ“‹ æ•°æ®æ ·æœ¬é¢„è§ˆï¼ˆå‰3æ¡ï¼‰:")
        for i, item in enumerate(dataset[:3]):
            text_preview = item['text'][:200] + "..." if len(item['text']) > 200 else item['text']
            print(f"\næ ·æœ¬ {i+1}:")
            print(f"  é•¿åº¦: {len(item['text'])} å­—ç¬¦")
            print(f"  å†…å®¹: {text_preview}")
        
        # é•¿åº¦åˆ†å¸ƒåˆ†æ
        lengths = [len(item['text']) for item in dataset]
        lengths.sort()
        
        print(f"\nğŸ“ˆ é•¿åº¦åˆ†å¸ƒç»Ÿè®¡:")
        print(f"  æœ€çŸ­: {min(lengths)} å­—ç¬¦")
        print(f"  æœ€é•¿: {max(lengths)} å­—ç¬¦")
        print(f"  ä¸­ä½æ•°: {lengths[len(lengths)//2]} å­—ç¬¦")
        print(f"  75%åˆ†ä½æ•°: {lengths[int(len(lengths)*0.75)]} å­—ç¬¦")
        print(f"  95%åˆ†ä½æ•°: {lengths[int(len(lengths)*0.95)]} å­—ç¬¦")
        
        return dataset
        
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶ {dataset_path}")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ é”™è¯¯ï¼šJSONè§£æå¤±è´¥ - {e}")
        return None
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šåŠ è½½æ•°æ®é›†æ—¶å‡ºç°å¼‚å¸¸ - {e}")
        return None

def prepare_for_training(dataset, output_file="processed_dataset/training_ready.json"):
    """
    ä¸ºè®­ç»ƒå‡†å¤‡æœ€ç»ˆçš„æ•°æ®é›†æ ¼å¼
    """
    print(f"\n=== å‡†å¤‡è®­ç»ƒæ•°æ® ===")
    
    # å‡†å¤‡è®­ç»ƒæ ¼å¼çš„æ•°æ®
    training_data = []
    for i, item in enumerate(dataset):
        training_item = {
            "id": i + 1,
            "text": item["text"],
            "length": len(item["text"]),
            "word_count": len(item["text"].split())
        }
        training_data.append(training_item)
    
    # ä¿å­˜è®­ç»ƒå‡†å¤‡æ•°æ®
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ¯ æ•°æ®æ ¼å¼: æ¯æ¡è®°å½•åŒ…å« id, text, length, word_count å­—æ®µ")
        return training_data
        
    except Exception as e:
        print(f"âŒ ä¿å­˜è®­ç»ƒæ•°æ®æ—¶å‡ºé”™: {e}")
        return None

if __name__ == "__main__":
    print("å¼€å§‹å¤„ç†text-chunks.jsonæ–‡ä»¶...")
    
    # å¤„ç†åŸºæœ¬æ•°æ®é›†
    process_chunks_json()
    
    print("\n" + "="*50)
    
    # ç”Ÿæˆè®­ç»ƒæ ·æœ¬
    print("ç”Ÿæˆæœºå™¨å­¦ä¹ è®­ç»ƒæ ·æœ¬...")
    generate_training_samples()
    
    print("\n" + "="*50)
    
    # åŠ è½½å¹¶éªŒè¯æ•°æ®é›†
    dataset = load_and_validate_dataset()
    
    if dataset:
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        training_data = prepare_for_training(dataset)
        
        if training_data:
            print(f"\nğŸ‰ æ•°æ®é›†å¤„ç†å®Œæˆï¼å¯ç”¨äºè®­ç»ƒçš„æ•°æ®é›†å·²å‡†å¤‡å°±ç»ªã€‚")
    
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶æ¸…å•ï¼š")
    print("- processed_dataset/combined_dataset.json      (ä¸»è¦è®­ç»ƒæ•°æ®)")
    print("- processed_dataset/qa_dataset.json           (é—®ç­”æ ¼å¼)")
    print("- processed_dataset/text_dataset.json         (çº¯æ–‡æœ¬æ ¼å¼)")
    print("- processed_dataset/training_text.txt         (æ–‡æœ¬æ–‡ä»¶)")
    print("- processed_dataset/training_samples.jsonl    (JSONLæ ¼å¼)")
    print("- processed_dataset/training_ready.json       (è®­ç»ƒå°±ç»ªæ•°æ®)")
    print("- processed_dataset/dataset_stats.json        (ç»Ÿè®¡ä¿¡æ¯)")
    
    print("\nğŸš€ æ•°æ®é›†å·²å‡†å¤‡å®Œæ¯•ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒï¼") 

    
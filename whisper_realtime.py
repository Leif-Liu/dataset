import whisper
import numpy as np
import sounddevice as sd
import queue
import threading
import sys

# é…ç½®å‚æ•°
SAMPLE_RATE = 16000  # Whisper ä½¿ç”¨ 16kHz é‡‡æ ·ç‡
CHUNK_DURATION = 5   # æ¯ 5 ç§’å¤„ç†ä¸€æ¬¡éŸ³é¢‘
CHUNK_SIZE = SAMPLE_RATE * CHUNK_DURATION

# åŠ è½½ Whisper æ¨¡å‹
print("æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹...")
model = whisper.load_model("turbo")
print("æ¨¡å‹åŠ è½½å®Œæˆï¼")

# éŸ³é¢‘é˜Ÿåˆ—
audio_queue = queue.Queue()

def audio_callback(indata, frames, time, status):
    """éŸ³é¢‘è¾“å…¥å›è°ƒå‡½æ•°"""
    if status:
        print(f"éŸ³é¢‘çŠ¶æ€: {status}", file=sys.stderr)
    # å°†éŸ³é¢‘æ•°æ®æ”¾å…¥é˜Ÿåˆ—
    audio_queue.put(indata.copy())

def transcribe_audio(audio_data):
    """è½¬å½•éŸ³é¢‘æ•°æ®"""
    try:
        # å°†éŸ³é¢‘æ•°æ®è½¬æ¢ä¸º float32 å¹¶å±•å¹³
        audio = audio_data.flatten().astype(np.float32)
        
        # æ–¹æ³•1: ä½¿ç”¨åº•å±‚ APIï¼ˆå½“å‰æ–¹æ³•ï¼‰
        # å¡«å……æˆ–è£å‰ªåˆ° 30 ç§’ï¼ˆWhisper çš„æ ‡å‡†é•¿åº¦ï¼‰
        # å¦‚æœéŸ³é¢‘ < 30ç§’ï¼Œä¼šåœ¨æœ«å°¾ç”¨é›¶å¡«å……ï¼›å¦‚æœ > 30ç§’ï¼Œä¼šè£å‰ª
        audio = whisper.pad_or_trim(audio)
        
        # ç”Ÿæˆ log-Mel é¢‘è°±å›¾
        mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)
        
        # æ£€æµ‹è¯­è¨€
        _, probs = model.detect_language(mel)
        detected_lang = max(probs, key=probs.get)
        
        # è§£ç éŸ³é¢‘
        options = whisper.DecodingOptions(language=detected_lang)
        result = whisper.decode(model, mel, options)
        
        return result.text, detected_lang
        
        # æ–¹æ³•2: ä½¿ç”¨é«˜çº§ APIï¼ˆæ›´æ¨èï¼Œè‡ªåŠ¨å¤„ç†ä»»æ„é•¿åº¦ï¼‰
        # result = model.transcribe(audio, language=None)
        # return result['text'], result['language']
        
    except Exception as e:
        print(f"è½¬å½•é”™è¯¯: {e}")
        return None, None

def process_audio():
    """å¤„ç†éŸ³é¢‘é˜Ÿåˆ—ä¸­çš„æ•°æ®"""
    audio_buffer = []
    
    while True:
        try:
            # ä»é˜Ÿåˆ—è·å–éŸ³é¢‘æ•°æ®
            chunk = audio_queue.get()
            audio_buffer.append(chunk)
            
            # å½“ç¼“å†²åŒºç´¯ç§¯åˆ°æŒ‡å®šæ—¶é•¿æ—¶è¿›è¡Œè½¬å½•
            if len(audio_buffer) * len(audio_buffer[0]) >= CHUNK_SIZE:
                # åˆå¹¶éŸ³é¢‘æ•°æ®
                audio_data = np.concatenate(audio_buffer)
                
                # è½¬å½•
                text, lang = transcribe_audio(audio_data)
                
                if text and text.strip():
                    print(f"\n[{lang}] {text}")
                    print("-" * 50)
                
                # æ¸…ç©ºç¼“å†²åŒºï¼ˆå¯ä»¥ä¿ç•™ä¸€äº›é‡å ä»¥æé«˜å‡†ç¡®æ€§ï¼‰
                audio_buffer = []
                
        except KeyboardInterrupt:
            break

def main():
    """ä¸»å‡½æ•°"""
    print(f"\nå¼€å§‹å®æ—¶è¯­éŸ³è½¬å½•...")
    print(f"é‡‡æ ·ç‡: {SAMPLE_RATE} Hz")
    print(f"å¤„ç†é—´éš”: {CHUNK_DURATION} ç§’")
    print(f"æŒ‰ Ctrl+C åœæ­¢\n")
    print("=" * 50)
    
    # å¯åŠ¨éŸ³é¢‘å¤„ç†çº¿ç¨‹
    processing_thread = threading.Thread(target=process_audio, daemon=True)
    processing_thread.start()
    
    try:
        # å¼€å§‹å½•éŸ³
        with sd.InputStream(
            samplerate=SAMPLE_RATE,
            channels=1,  # å•å£°é“
            callback=audio_callback,
            blocksize=int(SAMPLE_RATE * 0.5)  # æ¯ 0.5 ç§’å›è°ƒä¸€æ¬¡
        ):
            print("ğŸ¤ æ­£åœ¨ç›‘å¬éº¦å…‹é£...")
            # ä¿æŒè¿è¡Œ
            processing_thread.join()
    except KeyboardInterrupt:
        print("\n\nåœæ­¢å½•éŸ³...")
    except Exception as e:
        print(f"\né”™è¯¯: {e}")

if __name__ == "__main__":
    main()

